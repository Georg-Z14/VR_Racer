import cv2  # OpenCV: Zugriff auf Kamera, Bildverarbeitung (Lesen, Konvertieren, Resizing)
from aiortc import VideoStreamTrack  # aiortc-Basisklasse f√ºr einen WebRTC-Videostream
from av import VideoFrame  # PyAV: Kapselt ein einzelnes Video-Frame f√ºr aiortc
import numpy as np  # NumPy: Matrizen-/Array-Operationen, hier f√ºr Bilder
import threading  # Threads: Hintergrundausf√ºhrung f√ºr kontinuierliches Kameralesen


class MotionCameraStream(VideoStreamTrack):
    """üé• Kamera-Stream mit Bild-Resize und Bewegungserkennung."""  # Klassenbeschreibung/Docstring

    def __init__(self, camera_index=0, target_size=(1280, 720), sensitivity=40):
        super().__init__()  # Basisklassen-Konstruktor aufrufen (wichtig f√ºr aiortc-internen Status)

        # √ñffnet Kamera (z. B. USB-Kamera, CSI-Kamera oder Webcam)
        self.cap = cv2.VideoCapture(camera_index)  # Kamera-Handle erstellen; liefert sp√§ter Frames

        # Lock f√ºr Thread-Synchronisation (wichtig f√ºr gleichzeitigen Zugriff)
        self.lock = threading.Lock()  # Mutex f√ºr sicheren Zugriff auf gemeinsame Variablen (z. B. self.frame)
        self.prev_gray = None  # vorheriges Bild (f√ºr Bewegungserkennung) ‚Äì wird beim ersten Frame gesetzt
        self.motion_detected = False  # Statusflag: Ist Bewegung detektiert worden?
        self.sensitivity = sensitivity  # Empfindlichkeit der Bewegungserkennung (h√∂her = empfindlicher)
        self.running = True  # Kontrollflag f√ºr den Kamerathread; beendet die Leseschleife, wenn False

        # Wenn Kamera nicht ge√∂ffnet werden kann ‚Üí schwarzes Dummy-Bild
        if not self.cap.isOpened():  # Pr√ºfen, ob der Kamera-Treiber/Stream erfolgreich ge√∂ffnet wurde
            print(f"‚ùå Kamera mit Index {camera_index} konnte nicht ge√∂ffnet werden!")  # Fehlermeldung ausgeben
            self.frame = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)  # Fallback: schwarzes Frame
        else:
            # Kamera-Parameter setzen
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_size[0])  # Zielbreite setzen (sofern vom Treiber unterst√ºtzt)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, target_size[1])  # Zielh√∂he setzen
            self.cap.set(cv2.CAP_PROP_FPS, 30)  # Wunsch-Framerate (30 fps) anfragen
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Nur 1 Frame puffern ‚Üí geringere Latenz

        # Aktuelles Frame zwischenspeichern
        self.frame = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)  # Initialer Framepuffer (schwarz)
        self._target_w, self._target_h = target_size  # Zielgr√∂√üe f√ºr dynamisches Resizing merken

        # Startet eigenen Thread f√ºr kontinuierliches Lesen der Kamera
        self.thread = threading.Thread(target=self._reader, daemon=True)  # Hintergrundthread definieren (Daemon)
        self.thread.start()  # Thread starten ‚Üí ab jetzt werden fortlaufend Frames gelesen

    # ==============================================
    # üìè Aufl√∂sung dynamisch √§ndern
    # ==============================================
    def set_target_size(self, width: int, height: int):
        with self.lock:  # Sperre setzen: Threadsicheres Schreiben der Zielgr√∂√üe
            self._target_w, self._target_h = width, height  # Neue Zielbreite/-h√∂he √ºbernehmen

    def get_target_size(self):
        with self.lock:  # Sperre setzen: Threadsicheres Lesen der Zielgr√∂√üe
            return self._target_w, self._target_h  # Aktuelle Zielgr√∂√üe zur√ºckgeben

    # ==============================================
    # üß† Bewegungserkennung
    # ==============================================
    def _detect_motion(self, frame):
        """Vergleicht aktuelle Frames, um Bewegung zu erkennen."""  # Kurzbeschreibung der Methode
        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # Farbbild (RGB) in Graustufen umwandeln
        gray = cv2.GaussianBlur(gray, (21, 21), 0)  # Gau√ü-Blur: Rauschen gl√§tten, kleine Flackerer unterdr√ºcken

        # Erstes Bild als Referenz
        if self.prev_gray is None:  # Wenn noch kein Referenzbild vorhanden ist (erster Durchlauf)
            self.prev_gray = gray  # Referenz setzen
            return False  # Noch keine Bewegungsauswertung m√∂glich ‚Üí False zur√ºckgeben

        # Differenz zum vorherigen Bild
        diff = cv2.absdiff(self.prev_gray, gray)  # Absoluten Unterschied der Graubilder berechnen (Bewegungs-Indikator)

        # Schwellwert: gro√üe √Ñnderungen = Bewegung
        thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)[1]  # Binarisieren: √Ñnderungen > 25 werden wei√ü (255)
        motion_level = np.sum(thresh) / 255  # Anzahl ‚Äûwei√üer‚Äú Pixel (ge√§nderte Pixel) bestimmen

        # Referenzbild aktualisieren
        self.prev_gray = gray  # Aktuelles Bild als neue Referenz f√ºr den n√§chsten Vergleich speichern

        # Bewegung als erkannt markieren, wenn Schwelle √ºberschritten
        self.motion_detected = motion_level > self.sensitivity * 1000  # Einfacher Schwellwert-Vergleich

    # ==============================================
    # üîÑ Kamerathread ‚Äì liest Frames permanent
    # ==============================================
    def _reader(self):
        """Wird in eigenem Thread ausgef√ºhrt und h√§lt das neueste Kamerabild aktuell."""  # Beschreibung des Threads
        while self.running and self.cap.isOpened():  # Solange Stop nicht angefordert und Kamera offen ist
            ret, frame = self.cap.read()  # N√§chstes Frame aus der Kamera lesen (ret=True, wenn erfolgreich)
            if ret:  # Nur weiterarbeiten, wenn ein g√ºltiges Bild vorliegt
                # OpenCV liefert BGR, f√ºr Browser/WebRTC brauchen wir RGB
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Farbkan√§le von BGR nach RGB konvertieren

                # Bild ggf. auf Zielgr√∂√üe skalieren
                tw, th = self.get_target_size()  # Aktuelle Zielbreite/-h√∂he threadsicher holen
                if (frame.shape[1], frame.shape[0]) != (tw, th):  # Pr√ºfen, ob die Gr√∂√üe bereits passt
                    frame = cv2.resize(frame, (tw, th), interpolation=cv2.INTER_AREA)  # Auf Zielgr√∂√üe skalieren

                # Frame speichern (letztes g√ºltiges Bild)
                self.frame = frame  # Neues Frame als ‚Äûaktuelles‚Äú Frame ablegen (wird sp√§ter gesendet)

                # Bewegung pr√ºfen
                self._detect_motion(frame)  # Bewegungserkennung auf dem aktuellen Frame ausf√ºhren

    # ==============================================
    # üì° WebRTC Stream ‚Äì sendet Frames an Browser
    # ==============================================
    async def recv(self):
        """Wird von WebRTC aufgerufen, um das n√§chste Videoframe zu liefern."""  # aiortc-Hook f√ºr Frame-Abruf
        pts, time_base = await self.next_timestamp()  # Zeitstempel und Zeitbasis f√ºr das n√§chste Frame berechnen
        frm = VideoFrame.from_ndarray(self.frame, format="rgb24")  # NumPy-Array ‚Üí PyAV-VideoFrame (24-bit RGB)
        frm.pts = pts  # Pr√§sentationszeitstempel setzen (Synchronisation)
        frm.time_base = time_base  # Zeitbasis setzen (Einheit/Skalierung des PTS)
        return frm  # Frame an die WebRTC-Pipeline zur√ºckgeben

    # ==============================================
    # üßπ Kamera sauber beenden
    # ==============================================
    def stop(self):
        """Stoppt den Kamera-Thread und gibt Ressourcen frei."""  # √ñffentliche Methode zum Herunterfahren
        self.running = False  # Signal an den Lesethread: Schleife beenden
        if self.cap and self.cap.isOpened():  # Sicherstellen, dass ein Kamera-Handle existiert und offen ist
            self.cap.release()  # Kamera freigeben (sonst bleibt das Device blockiert)
            print("üì∑ Kamera gestoppt")  # Best√§tigung in der Konsole ausgeben